{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b016cb6-7e7b-47ac-9fbc-1cbf07e5fafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: 200\n",
      "Scraping page 2: 200\n",
      "Scraping page 3: 200\n",
      "Scraping page 4: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 4: 200\n",
      "Scraping page 5: 200\n",
      "Scraping page 6: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 6: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 6: 200\n",
      "Scraping page 7: 200\n",
      "Scraping page 8: 200\n",
      "Scraping page 9: 200\n",
      "Scraping page 10: 200\n",
      "Scraping page 11: 200\n",
      "Scraping page 12: 200\n",
      "Scraping page 13: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 13: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 13: 200\n",
      "Scraping page 14: 200\n",
      "Scraping page 15: 200\n",
      "Scraping page 16: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 16: 200\n",
      "Scraping page 17: 200\n",
      "Scraping page 18: 200\n",
      "Scraping page 19: 200\n",
      "Scraping page 20: 200\n",
      "Scraping page 21: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 21: 200\n",
      "Scraping page 22: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 22: 200\n",
      "Scraping page 23: 200\n",
      "Scraping page 24: 200\n",
      "Scraping page 25: 200\n",
      "Scraping page 26: 200\n",
      "Scraping page 27: 200\n",
      "Scraping page 28: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 28: 200\n",
      "Scraping page 29: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 29: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 29: 200\n",
      "Scraping page 30: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 30: 200\n",
      "Scraping page 31: 200\n",
      "Scraping page 32: 200\n",
      "Scraping page 33: 200\n",
      "Scraping page 34: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 34: 200\n",
      "Scraping page 35: 200\n",
      "Scraping page 36: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 36: 200\n",
      "Scraping page 37: 200\n",
      "Scraping page 38: 200\n",
      "Scraping page 39: 200\n",
      "Scraping page 40: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 40: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 40: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 40: 200\n",
      "Scraping page 41: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 41: 200\n",
      "Scraping page 42: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 42: 200\n",
      "Scraping page 43: 200\n",
      "Scraping page 44: 200\n",
      "Scraping page 45: 200\n",
      "Scraping page 46: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 46: 200\n",
      "Scraping page 47: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 47: 200\n",
      "Scraping page 48: 200\n",
      "Scraping page 49: 200\n",
      "Scraping page 50: 200\n",
      "Scraping page 51: 200\n",
      "Scraping page 52: 200\n",
      "Scraping page 53: 200\n",
      "Scraping page 54: 200\n",
      "Scraping page 55: 200\n",
      "Scraping page 56: 200\n",
      "Scraping page 57: 200\n",
      "Scraping page 58: 200\n",
      "Scraping page 59: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 59: 200\n",
      "Scraping page 60: 200\n",
      "Scraping page 61: 200\n",
      "Scraping page 62: 200\n",
      "Scraping page 63: 200\n",
      "Scraping page 64: 200\n",
      "Scraping page 65: 200\n",
      "Scraping page 66: 200\n",
      "Scraping page 67: 200\n",
      "Scraping page 68: 200\n",
      "Scraping page 69: 200\n",
      "Scraping page 70: 200\n",
      "Scraping page 71: 200\n",
      "Scraping page 72: 200\n",
      "Scraping page 73: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 73: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 73: 200\n",
      "Scraping page 74: 200\n",
      "Scraping page 75: 200\n",
      "Scraping page 76: 200\n",
      "Scraping page 77: 200\n",
      "Scraping page 78: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 78: 200\n",
      "Scraping page 79: 200\n",
      "Scraping page 80: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 80: 200\n",
      "Scraping page 81: 200\n",
      "Scraping page 82: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 82: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 82: 200\n",
      "Scraping page 83: 200\n",
      "Scraping page 84: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 84: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 84: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 84: 200\n",
      "Scraping page 85: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 85: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 85: 200\n",
      "Scraping page 86: 200\n",
      "Scraping page 87: 200\n",
      "Scraping page 88: 200\n",
      "Scraping page 89: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 89: 200\n",
      "Scraping page 90: 200\n",
      "Scraping page 91: 200\n",
      "Scraping page 92: 200\n",
      "Scraping page 93: 200\n",
      "Scraping page 94: 200\n",
      "Scraping page 95: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 95: 200\n",
      "Scraping page 96: 200\n",
      "Scraping page 97: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 97: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 97: 200\n",
      "Scraping page 98: 200\n",
      "Scraping page 99: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 99: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 99: 200\n",
      "Scraping page 100: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 100: 200\n",
      "Scraping page 101: 200\n",
      "Scraping page 102: 200\n",
      "Scraping page 103: 200\n",
      "Scraping page 104: 200\n",
      "Scraping page 105: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 105: 200\n",
      "Scraping page 106: 200\n",
      "Scraping page 107: 200\n",
      "Scraping page 108: 200\n",
      "Scraping page 109: 200\n",
      "Scraping page 110: 200\n",
      "Scraping page 111: 200\n",
      "Scraping page 112: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 112: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 112: 200\n",
      "Scraping page 113: 200\n",
      "Scraping page 114: 200\n",
      "Scraping page 115: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 115: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 115: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 115: 200\n",
      "Scraping page 116: 200\n",
      "Scraping page 117: 200\n",
      "Scraping page 118: 200\n",
      "Scraping page 119: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 119: 200\n",
      "Scraping page 120: 200\n",
      "Scraping page 121: 200\n",
      "Scraping page 122: 200\n",
      "Scraping page 123: 200\n",
      "Scraping page 124: 200\n",
      "Scraping page 125: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 125: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 125: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 125: 200\n",
      "Scraping page 126: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 126: 200\n",
      "Scraping page 127: 200\n",
      "Scraping page 128: 200\n",
      "Scraping page 129: 200\n",
      "Scraping page 130: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 130: 200\n",
      "Scraping page 131: 200\n",
      "Scraping page 132: 200\n",
      "Scraping page 133: 200\n",
      "Scraping page 134: 200\n",
      "Scraping page 135: 200\n",
      "Scraping page 136: 200\n",
      "Scraping page 137: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 137: 200\n",
      "Scraping page 138: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 138: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 138: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 138: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 138: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Rate limit not overcome. Exiting the loop.\n",
      "Length of Product_name: 960\n",
      "Length of Prices: 958\n",
      "Length of Description: 960\n",
      "Length of Reviews: 944\n",
      "Data scraping completed and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd  # Import pandas for data manipulation and analysis\n",
    "import requests  # Import requests for sending HTTP requests\n",
    "from bs4 import BeautifulSoup  # Import BeautifulSoup from bs4 for parsing HTML and XML documents\n",
    "import csv  # Import csv for handling CSV files\n",
    "import time  # Import time to add delay between requests to avoid rate limiting\n",
    "import random  # Import random to generate random sleep times between requests\n",
    "\n",
    "# Define headers to mimic a browser request to avoid being blocked by the website\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Initialize lists outside the loop to accumulate data across pages\n",
    "Product_name = []  # List to store product names\n",
    "Prices = []  # List to store product prices\n",
    "Description = []  # List to store product descriptions\n",
    "Reviews = []  # List to store product reviews\n",
    "\n",
    "# Loop through 1 to 99 to scrape data from multiple pages\n",
    "for i in range(1, 150):\n",
    "    # Construct the URL with the page number i\n",
    "    url = f\"https://www.flipkart.com/search?q=mobiles+under+50000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&as-pos=1&as-type=HISTORY&page={i}\"\n",
    "    \n",
    "    # Attempt to fetch the page with retries to handle potential rate limits or errors\n",
    "    for attempt in range(7):\n",
    "        page = requests.get(url, headers=headers)  # Send a GET request to the specified URL with headers\n",
    "        print(f\"Scraping page {i}: {page.status_code}\")  # Print the status code to know the result of the request\n",
    "        \n",
    "        if page.status_code == 429:\n",
    "            # If the status code is 429 (Too Many Requests), wait before retrying\n",
    "            print(\"Rate limit hit. Waiting before retrying...\")\n",
    "            time.sleep(10 + attempt * 7)  # Exponential backoff: wait longer with each attempt\n",
    "            continue  # Retry the request\n",
    "        elif page.status_code == 200:\n",
    "            # If the status code is 200 (OK), the request was successful\n",
    "            break  # Exit the retry loop\n",
    "        else:\n",
    "            # If the status code is not 200 or 429, handle it as an unexpected status\n",
    "            print(f\"Unexpected status code {page.status_code}. Stopping the loop.\")\n",
    "            break  # Exit both loops\n",
    "      # If the status is still 429 after retries, stop the entire scraping process\n",
    "    if page.status_code == 429:\n",
    "        print(\"Rate limit not overcome. Exiting the loop.\")\n",
    "        break  # Exit the main loop\n",
    "    \n",
    "    # Get the content of the page from the response\n",
    "    fd = page.content\n",
    "    \n",
    "    # Parse the HTML content of the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(fd, 'html.parser')\n",
    "    \n",
    "    # Find the main container that holds the product listings\n",
    "    box = soup.find('div', class_='DOjaWF gdgoEp')\n",
    "    \n",
    "    # Check if the box is None (i.e., no products found on this page)\n",
    "    if box is None:\n",
    "        print(f\"No products found on page {i}. Stopping the loop.\")\n",
    "        break  # Stop scraping further pages if no products are found\n",
    "    \n",
    "     \n",
    "    # Finding and storing Product Names\n",
    "    names = box.find_all('div', class_='KzDlHZ')  # Find all elements with the product name class\n",
    "    for name in names:\n",
    "        Product_name.append(name.text)  # Append the text content of each name to the Product_name list\n",
    "    \n",
    "    # Finding and storing Product Prices\n",
    "    prices = box.find_all('div', class_='Nx9bqj _4b5DiR')  # Find all elements with the product price class\n",
    "    for price in prices:\n",
    "        Prices.append(price.text.strip())  # Append the cleaned (stripped) text of each price to the Prices list\n",
    "\n",
    "    # Finding and storing Product Descriptions\n",
    "    desc = box.find_all('ul', class_='G4BRas')  # Find all elements with the product description class\n",
    "    for description in desc:\n",
    "        Description.append(description.text)  # Append the text content of each description to the Description list\n",
    "    \n",
    "    # Finding and storing Product Reviews\n",
    "    reviews = box.find_all('div', class_='XQDdHH')  # Find all elements with the product review class\n",
    "    for review in reviews:\n",
    "        Reviews.append(review.text)  # Append the text content of each review to the Reviews list\n",
    "    \n",
    "\n",
    "    # Random sleep time between requests to avoid getting blocked by the website's rate limiting\n",
    "    time.sleep(random.uniform(2, 7))  # Sleep for a random duration between 2 and 5 seconds\n",
    "\n",
    "# After scraping all pages, ensure data consistency\n",
    "\n",
    "# Check the lengths of the lists to verify that they are equal\n",
    "print(f\"Length of Product_name: {len(Product_name)}\")\n",
    "print(f\"Length of Prices: {len(Prices)}\")\n",
    "print(f\"Length of Description: {len(Description)}\")\n",
    "print(f\"Length of Reviews: {len(Reviews)}\")\n",
    "\n",
    "# Ensure all lists have the same length by truncating them to the length of the shortest list\n",
    "min_length = min(len(Product_name), len(Prices), len(Description), len(Reviews))\n",
    "\n",
    "Product_name = Product_name[:min_length]  # Truncate the Product_name list to the minimum length\n",
    "Prices = Prices[:min_length]  # Truncate the Prices list to the minimum length\n",
    "Description = Description[:min_length]  # Truncate the Description list to the minimum length\n",
    "Reviews = Reviews[:min_length]  # Truncate the Reviews list to the minimum length\n",
    "\n",
    "# Create a DataFrame to organize the data\n",
    "df = pd.DataFrame({\n",
    "    'Product Name': Product_name,  # Add the Product_name list as a column named 'Product Name'\n",
    "    'Prices': Prices,  # Prepend 'â‚¹' to each price and add it as a column named 'Prices'\n",
    "    'Description': Description,  # Add the Description list as a column named 'Description'\n",
    "    'Reviews': Reviews  # Add the Reviews list as a column named 'Reviews'\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"E:/#DATA_ANALYST_PORTFOLIO_PROJECTS/03_PYTHON/Web_Scrapping/Flipkart_Multiple_Page_Scrapper/Flipkart_mobiles_under_50000.csv\", \n",
    "          index=False, encoding='utf-8-sig')  # Save the DataFrame to a CSV file without the index and with UTF-8 encoding\n",
    "\n",
    "print(\"Data scraping completed and saved to CSV.\")  # Indicate that the scraping and saving process is complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2462322-26c6-4401-a843-36cddcffd3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

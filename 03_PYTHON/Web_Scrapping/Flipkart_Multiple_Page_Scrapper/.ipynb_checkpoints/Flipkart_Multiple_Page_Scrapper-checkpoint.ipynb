{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbce8bc6-f31e-4114-8a29-cf1d7f2107de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5685f4ad-a587-4f36-9fc2-9549f2d71b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.flipkart.com/mobile-phones-store?fm=neo%2Fmerchandising&iid=M_bf04ee8e-618d-4921-9af9-06592ffcf38e_1_KUZ8W60OFFMO_MC.BYIXDBQAWBHQ&otracker=hp_rich_navigation_2_1.navigationCard.RICH_NAVIGATION_Mobiles%2B%2526%2BTablets_BYIXDBQAWBHQ&otracker1=hp_rich_navigation_PINNED_neo%2Fmerchandising_NA_NAV_EXPANDABLE_navigationCard_cc_2_L0_view-all&cid=BYIXDBQAWBHQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d8efc0-37b3-4c6b-b52a-0b3e5da07d14",
   "metadata": {},
   "outputs": [],
   "source": [
    " # r = requests.get(url)\n",
    " # print(r)\n",
    "# headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "# r = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63017e1d-dcf2-4e98-ac93-ee3f583a18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(r.text, \"lxml\")\n",
    "# # print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cd398ad-8b11-48b0-9d76-53a5bdb8524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np = soup.find(\"a\",class_ = \"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\").get(\"href\")\n",
    "# print(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8f82c8d7-8496-4895-add1-76e0260c3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Attempt to find the next page link\n",
    "# next_page = soup.find(\"a\", class_=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\")\n",
    "\n",
    "# # Check if the link is found\n",
    "# if next_page:\n",
    "#     np_href = next_page.get(\"href\")\n",
    "#     print(\"Next page link:\", np_href)\n",
    "# else:\n",
    "#     print(\"Next page link not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "604d438d-9e7f-4435-a150-3d2660f9114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [429]>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[376], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m     Reviews \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Finding Product Names\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[43mbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m,class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKzDlHZ\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m names:\n\u001b[0;32m     39\u001b[0m         name \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#Multiple Page\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup  \n",
    "import csv  \n",
    "from pprint import pprint  \n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1,30):\n",
    "    url = \"https://www.flipkart.com/search?q=mobiles+under+50000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&as-pos=1&as-type=HISTORY&page=\"+str(i)\n",
    "    page = requests.get(url, headers=headers)\n",
    "    print(page)\n",
    "    fd = page.content\n",
    "    # pprint(fd[:500])\n",
    "    soup = BeautifulSoup(fd, 'html.parser')\n",
    "    soup_str = str(soup)\n",
    "    # next_page = soup.find('a', class_='_9QVEpD').get('href')\n",
    "    # complete_next_page = 'https://www.flipkart.com' + next_page\n",
    "    # print(complete_next_page)\n",
    "    \n",
    "    box = soup.find('div', class_='DOjaWF gdgoEp')\n",
    "    # We did it in box beacuse some extra reviews from bottom were coming, so we wanted review only from the top 25 searches.\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------\n",
    "    Product_name = []\n",
    "    Prices = []\n",
    "    Description = []\n",
    "    Reviews = []\n",
    "\n",
    "# Finding Product Names\n",
    "    \n",
    "    names = box.find_all('div',class_='KzDlHZ')\n",
    "    \n",
    "    for i in names:\n",
    "        name = i.text\n",
    "        Product_name.append(name)\n",
    "    \n",
    "    # print(Product_name)\n",
    "    \n",
    "    # Finding Product Prices\n",
    "    \n",
    "    prices = box.find_all('div',class_='Nx9bqj _4b5DiR')\n",
    "    \n",
    "    for i in prices:\n",
    "        price = i.text\n",
    "        Prices.append(price)\n",
    "    \n",
    "    # print(Prices)\n",
    "    \n",
    "    # Finding Product Descriptions\n",
    "    \n",
    "    desc = box.find_all('ul',class_='G4BRas')\n",
    "    \n",
    "    for i in desc:\n",
    "        description = i.text\n",
    "        Description.append(description)\n",
    "    \n",
    "    # print(Description)\n",
    "    \n",
    "    # Finding Product Reviews\n",
    "    \n",
    "    reviews = box.find_all('div',class_='XQDdHH')\n",
    "    \n",
    "    for i in reviews:\n",
    "        review = i.text\n",
    "        Reviews.append(review)\n",
    "\n",
    "# print(Reviews)\n",
    "# print(len(Reviews))\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Adding it to a Dataframe\n",
    "\n",
    "df = pd.DataFrame({'Product Name':Product_name, 'Prices':Prices, 'Description':Description, 'Reviews':Reviews})\n",
    "# print(df)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "df.to_csv(\"E:/#DATA_ANALYST_PORTFOLIO_PROJECTS/03_PYTHON/Web_Scrapping/Flipkart_Multiple_Page_Scrapper/Flipkart_mobiles_under_50000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e48b8ca6-4604-4283-9599-3e9f48dc9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pprint(soup_str[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2e14704b-5ff4-49e5-8192-f2f3ae97576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.flipkart.com/search?q=mobiles+under+50000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&as-pos=1&as-type=HISTORY&page=2\n",
      "https://www.flipkart.com/search?q=mobiles+under+50000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&as-pos=1&as-type=HISTORY&page=1\n",
      "https://www.flipkart.com/search?q=mobiles+under+50000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&as-pos=1&as-type=HISTORY&page=2\n",
      "https://www.flipkart.com/search?q=mobiles+under+50000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&as-pos=1&as-type=HISTORY&page=1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     next_page \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_9QVEpD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     complete_next_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m next_page\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(complete_next_page)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Will work for websites other than flipkart\n",
    "# while True:\n",
    "#     next_page = soup.find('a', class_='_9QVEpD').get('href')\n",
    "#     complete_next_page = 'https://www.flipkart.com' + next_page\n",
    "#     print(complete_next_page)\n",
    "    \n",
    "#     url = complete_next_page\n",
    "#     page = requests.get(url, headers=headers)\n",
    "#     fd = page.content\n",
    "#     soup = BeautifulSoup(fd, 'html.parser')\n",
    "#     soup_str = str(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7ac23-85df-47fe-a029-bb4eeb005ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb955d-ee08-4752-842b-85d0ac6820d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61311700-6094-48b0-95f9-12ed94f38219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "52e610e5-b956-4698-b7c2-3a5e2c7f6940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: 200\n",
      "Scraping page 2: 200\n",
      "Scraping page 3: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 3: 200\n",
      "Scraping page 4: 200\n",
      "Scraping page 5: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 5: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 5: 200\n",
      "Scraping page 6: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 6: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 6: 200\n",
      "Scraping page 7: 200\n",
      "Scraping page 8: 200\n",
      "Scraping page 9: 200\n",
      "Scraping page 10: 200\n",
      "Scraping page 11: 200\n",
      "Scraping page 12: 200\n",
      "Scraping page 13: 200\n",
      "Scraping page 14: 200\n",
      "Scraping page 15: 200\n",
      "Scraping page 16: 200\n",
      "Scraping page 17: 200\n",
      "Scraping page 18: 200\n",
      "Scraping page 19: 200\n",
      "Scraping page 20: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 20: 200\n",
      "Scraping page 21: 200\n",
      "Scraping page 22: 200\n",
      "Scraping page 23: 200\n",
      "Scraping page 24: 200\n",
      "Scraping page 25: 200\n",
      "Scraping page 26: 200\n",
      "Scraping page 27: 200\n",
      "Scraping page 28: 200\n",
      "Scraping page 29: 200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[384], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# After the loop, save all the data into a CSV file\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProduct Name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mProduct_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReviews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mReviews\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/#DATA_ANALYST_PORTFOLIO_PROJECTS/03_PYTHON/Web_Scrapping/Flipkart_Multiple_Page_Scrapper/Flipkart_mobiles_under_50000.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData scraping completed and saved to CSV.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Initialize lists outside the loop to accumulate data across pages\n",
    "Product_name = []\n",
    "Prices = []\n",
    "Description = []\n",
    "Reviews = []\n",
    "\n",
    "for i in range(1, 30):\n",
    "    url = f\"https://www.flipkart.com/search?q=mobiles+under+50000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&as-pos=1&as-type=HISTORY&page={i}\"\n",
    "    \n",
    "    # Attempt to fetch the page with retries\n",
    "    for attempt in range(5):\n",
    "        page = requests.get(url, headers=headers)\n",
    "        print(f\"Scraping page {i}: {page.status_code}\")\n",
    "        \n",
    "        if page.status_code == 429:\n",
    "            print(\"Rate limit hit. Waiting before retrying...\")\n",
    "            time.sleep(10 + attempt * 5)  # Exponential backoff\n",
    "            continue\n",
    "        elif page.status_code == 200:\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Unexpected status code {page.status_code}. Stopping the loop.\")\n",
    "            break\n",
    "    \n",
    "    # If status is still 429 after retries, stop the loop\n",
    "    if page.status_code == 429:\n",
    "        print(\"Rate limit not overcome. Exiting the loop.\")\n",
    "        break\n",
    "    \n",
    "    fd = page.content\n",
    "    soup = BeautifulSoup(fd, 'html.parser')\n",
    "    \n",
    "    box = soup.find('div', class_='DOjaWF gdgoEp')\n",
    "    \n",
    "    # Check if the box is None (i.e., no products found on this page)\n",
    "    if box is None:\n",
    "        print(f\"No products found on page {i}. Stopping the loop.\")\n",
    "        break\n",
    "    \n",
    "    # Finding Product Names\n",
    "    names = box.find_all('div', class_='KzDlHZ')\n",
    "    for name in names:\n",
    "        Product_name.append(name.text)\n",
    "    \n",
    "    # Finding Product Prices\n",
    "    prices = box.find_all('div', class_='Nx9bqj _4b5DiR')\n",
    "    for price in prices:\n",
    "        Prices.append(price.text)\n",
    "    \n",
    "    # Finding Product Descriptions\n",
    "    desc = box.find_all('ul', class_='G4BRas')\n",
    "    for description in desc:\n",
    "        Description.append(description.text)\n",
    "    \n",
    "    # Finding Product Reviews\n",
    "    reviews = box.find_all('div', class_='XQDdHH')\n",
    "    for review in reviews:\n",
    "        Reviews.append(review.text)\n",
    "\n",
    "    # Random sleep time between requests to avoid rate limiting\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "# After the loop, save all the data into a CSV file\n",
    "df = pd.DataFrame({\n",
    "    'Product Name': Product_name, \n",
    "    'Prices': Prices, \n",
    "    'Description': Description, \n",
    "    'Reviews': Reviews\n",
    "})\n",
    "\n",
    "df.to_csv(\"E:/#DATA_ANALYST_PORTFOLIO_PROJECTS/03_PYTHON/Web_Scrapping/Flipkart_Multiple_Page_Scrapper/Flipkart_mobiles_under_50000.csv\", index=False)\n",
    "\n",
    "print(\"Data scraping completed and saved to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d5f0e-4a72-4abd-9e49-a5af7d31305c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "4747f2a0-bbaf-4a16-9106-923985f6086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: 200\n",
      "Scraping page 2: 200\n",
      "Scraping page 3: 200\n",
      "Scraping page 4: 200\n",
      "Scraping page 5: 200\n",
      "Scraping page 6: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 6: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 6: 200\n",
      "Scraping page 7: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 7: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 7: 200\n",
      "Scraping page 8: 200\n",
      "Scraping page 9: 200\n",
      "Scraping page 10: 200\n",
      "Scraping page 11: 200\n",
      "Scraping page 12: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 12: 200\n",
      "Scraping page 13: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 13: 200\n",
      "Scraping page 14: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 14: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 14: 200\n",
      "Scraping page 15: 200\n",
      "Scraping page 16: 200\n",
      "Scraping page 17: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 17: 200\n",
      "Scraping page 18: 200\n",
      "Scraping page 19: 200\n",
      "Scraping page 20: 200\n",
      "Scraping page 21: 200\n",
      "Scraping page 22: 200\n",
      "Scraping page 23: 200\n",
      "Scraping page 24: 200\n",
      "Scraping page 25: 200\n",
      "Scraping page 26: 200\n",
      "Scraping page 27: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 27: 200\n",
      "Scraping page 28: 200\n",
      "Scraping page 29: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 29: 200\n",
      "Length of Product_name: 672\n",
      "Length of Prices: 672\n",
      "Length of Description: 672\n",
      "Length of Reviews: 659\n",
      "Data scraping completed and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Initialize lists outside the loop to accumulate data across pages\n",
    "Product_name = []\n",
    "Prices = []\n",
    "Description = []\n",
    "Reviews = []\n",
    "\n",
    "for i in range(1, 100):\n",
    "    url = f\"https://www.flipkart.com/search?q=mobiles+under+50000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&as-pos=1&as-type=HISTORY&page={i}\"\n",
    "    \n",
    "    # Attempt to fetch the page with retries\n",
    "    for attempt in range(5):\n",
    "        page = requests.get(url, headers=headers)\n",
    "        print(f\"Scraping page {i}: {page.status_code}\")\n",
    "        \n",
    "        if page.status_code == 429:\n",
    "            print(\"Rate limit hit. Waiting before retrying...\")\n",
    "            time.sleep(10 + attempt * 5)  # Exponential backoff\n",
    "            continue\n",
    "        elif page.status_code == 200:\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Unexpected status code {page.status_code}. Stopping the loop.\")\n",
    "            break\n",
    "    \n",
    "    # If status is still 429 after retries, stop the loop\n",
    "    if page.status_code == 429:\n",
    "        print(\"Rate limit not overcome. Exiting the loop.\")\n",
    "        break\n",
    "    \n",
    "    fd = page.content\n",
    "    soup = BeautifulSoup(fd, 'html.parser')\n",
    "    \n",
    "    box = soup.find('div', class_='DOjaWF gdgoEp')\n",
    "    \n",
    "    # Check if the box is None (i.e., no products found on this page)\n",
    "    if box is None:\n",
    "        print(f\"No products found on page {i}. Stopping the loop.\")\n",
    "        break\n",
    "    \n",
    "    # Finding Product Names\n",
    "    names = box.find_all('div', class_='KzDlHZ')\n",
    "    for name in names:\n",
    "        Product_name.append(name.text)\n",
    "    \n",
    "    # Finding Product Prices\n",
    "    prices = box.find_all('div', class_='Nx9bqj _4b5DiR')\n",
    "    for price in prices:\n",
    "        Prices.append(price.text.strip())\n",
    "\n",
    "    # Finding Product Descriptions\n",
    "    desc = box.find_all('ul', class_='G4BRas')\n",
    "    for description in desc:\n",
    "        Description.append(description.text)\n",
    "    \n",
    "    # Finding Product Reviews\n",
    "    reviews = box.find_all('div', class_='XQDdHH')\n",
    "    for review in reviews:\n",
    "        Reviews.append(review.text)\n",
    "\n",
    "    # Random sleep time between requests to avoid rate limiting\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "# After scraping all pages\n",
    "\n",
    "# Check the lengths of the lists\n",
    "print(f\"Length of Product_name: {len(Product_name)}\")\n",
    "print(f\"Length of Prices: {len(Prices)}\")\n",
    "print(f\"Length of Description: {len(Description)}\")\n",
    "print(f\"Length of Reviews: {len(Reviews)}\")\n",
    "\n",
    "# Ensure all lists have the same length\n",
    "min_length = min(len(Product_name), len(Prices), len(Description), len(Reviews))\n",
    "\n",
    "Product_name = Product_name[:min_length]\n",
    "Prices = Prices[:min_length]\n",
    "Description = Description[:min_length]\n",
    "Reviews = Reviews[:min_length]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Product Name': Product_name, \n",
    "    'Prices': ['â‚¹' + price for price in Prices], \n",
    "    'Description': Description, \n",
    "    'Reviews': Reviews\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"E:/#DATA_ANALYST_PORTFOLIO_PROJECTS/03_PYTHON/Web_Scrapping/Flipkart_Multiple_Page_Scrapper/Flipkart_mobiles_under_50000.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Data scraping completed and saved to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebebb854-c0f3-48d5-8b7e-c1e4c22e7bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
